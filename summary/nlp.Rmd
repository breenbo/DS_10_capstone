---
title: "Text Prediction"
output: 
    html_document: 
        includes:
            after_body: custom/footer.html
        toc: true 
        toc_depth: 3
        toc_float: true
        highlight: pygments
        css: custom/custom.css
---


```{r globalOptions, echo=FALSE}
knitr::opts_chunk$set(comment=NA, message=FALSE)
```
```{r, libraries}
library(stringr) # wordcount
library(tm)
library(SnowballC)
library(RWeka) # n-grams
library(wordcloud2) # wordcloud
library(wordcloud)
library(knitr)
library(tidytext)
library(dplyr)
library(data.table)
library(wordcloud)
library(RColorBrewer)
library(tidyr)
library(igraph)
library(ggraph)
```
```{r, functionResume, echo=FALSE}
# quick overwiew of datas

# overview for multiple files
multiOverview <- function(liste) {
    # take a list of character containing names of text variables, then return a dataframe of stats
    dataSumList <- lapply(liste, function(x) overview(get(x)))
    len <- length(liste)
    dataSum <- data.frame()
    for(i in 1:len) {
        dataSum <- rbind(dataSum, dataSumList[[i]])
    }
    row.names(dataSum) <- liste
    dataSum
}

# preprocess text file


tdmNgram <- function(corpus, nb=2){
    # take a corpus, min and max length of ngram, and return a tdm of ngrams
    library(tm)
    library(RWeka)
    options(mc.cores=1)
    ngramControl <- function(x) {
        RWeka::NGramTokenizer(x, Weka_control(min=nb, max=nb))
    }
    tdm <- TermDocumentMatrix(corpus, control=list(tokenize=ngramControl))
    tdm
}
```

# Principles

1. "Text mining" : download and clean the dataset for analysis.
      - predictive machine isn't text mining for ordering or analysis
      - question for stemming and remove stopwords, because people write them so they need predictive dictionary

2. Exploratory Analysis : 
      - understand the distribution and the relationship between the words, tokens and phrases.
      - understand basic relationship in the data
      - prepare to build the first linguistic model

# Text Mining 

## Theory

- initial text
- preprocessing - cleaning corpus
    + all to lower
    + remove punctuation, numbers, url,...
    + remove stopwords
    + remove whitespaces
    + remove word less than 2 char
    + Stemming / Lemming
- Tokenization (1 and n-grams)
- keywords lexical

## Decision to make

1. sample datasets ?
1. Separate datasets ?
      - twit are short message with lots of typo, and news / blogs are more developed text
      - train model on different purposes, for different users.  
        

1. Remove stopwords ?
      - Stopwords useful for prediction typing algo
      - Stopwords to remove for sorting algo


1. Stemming or not ?
      - Good to unite declinaison of words, and remove some typos

# Packages {.tabset .tabset-fade .tabset-pills}

## Create corpus
***

```{r, readData, cache=TRUE}
library(tm)
twit <- readLines("../text/en_US.twitter.txt", skipNul=TRUE)
```
Quick exploratory :
```{r, overview, results="asis", cache=TRUE}
# function to view some stats
# str_count(texte, '\\w+') to count words in texte
overview <- function(texte) {
    data.frame(total.Lines=length(texte), total.words=sum(str_count(texte,'\\w+')), mean.word.by.line=mean(str_count(texte,'\\w+')), sd.of.mean=sd(str_count(texte,'\\w+')), max.word.by.line=max(str_count(texte,'\\w+')))
}
dataSum <- overview(twit)

# view results
library(knitr)
knitr::kable(dataSum)
```
```{r, sample, cache=TRUE}
twit[1]
```

Searching some words (for quizz, but can be useful in other cases).
Use *grep()* function :
```{r, question3, cache=TRUE}
length(twit)
nbLinesLove <- length(grep("love", twit))
nbLinesHate <- length(grep("hate", twit))
nbLinesLove / nbLinesHate
twit[grep("biostats", twit)]
grep("A computer once beat me at chess, but it was no match for me at kickboxing", twit)
```

## tm package
***

### Sources

- [data.bzh](http://data-bzh.fr/?s=text+mining)
- [whatsupdata.fr](http://whatsupdata.fr/content/le-text-mining-et-la-d%C3%A9couverte-des-principales-%C2%AB-th%C3%A9matiques-%C2%BB-d%E2%80%99un-corpus-de-documents)
- [gentle introduction to text mining using R](https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/)
- [gentle introduction to topic modeling using R](https://eight2late.wordpress.com/?s=topic+model)
- [R code : reading, preprocessing and counting](http://www.katrinerk.com/courses/words-in-a-haystack-an-introductory-statistics-course/schedule-words-in-a-haystack/r-code-the-text-mining-package)


### Pre-processing
Select random sample for testing process and functions :
```{r, testCorpus, cache=TRUE}
library(tm)
set.seed(2704)
twitTest <- sample(twit, 2000, replace=F)
```
Unecessary items in text mining are : punctuation, numbers, url, etc.  
To have the transformations possible with tm package :
```{r}
getTransformations()
```

```{r}
cleanCorpus <- function(textFile, lang="english", stopwords=FALSE, stemming=FALSE) {
    # take a text file or a text variable, and a language, and return a clean and stemmed corpus 
    library(tm)
    library(SnowballC)
    if(length(textFile)==1) {
        # textFile is a path to file must be read
        texte <- readLines(textFile, skipNul=TRUE)
    } else {
        # textFile is a variable already read
        texte <- textFile
    }
    # create corpus with file
    corpus <- VCorpus(VectorSource(texte))
    # clean and stem corpus
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeNumbers)
    if(stopwords==TRUE){
        corpus <- tm_map(corpus, removeWords, stopwords(lang))
    }
    if(stemming==TRUE){
        corpus <- tm_map(corpus, stemDocument, language=lang)
    }
    corpus <- tm_map(corpus, stripWhitespace)
    corpus
}

twitClean <- cleanCorpus(twitTest, stopwords=TRUE, stemming=TRUE)
print(object.size(twitClean), units="auto", quote=FALSE)
```

Possibility to remove specific words :
```{r, eval=FALSE}
twitClean <- tm_map(twitClean, removeWords, c("one", "two"))
```

Lemming - more accurate ?

- class words on lemm (run ran running -> run), based on the signification
- use packages [TreeTagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/#Linux) and [koRpus](http://edutechwiki.unige.ch/fr/Tutoriel_koRpus).
- don't work, don't know why...

To illustrate before and after the processing :
```{r, compareResults, results="asis"}
result <- data.frame(before=twitTest[[27]], after=twitClean[[27]]$content)
knitr::kable(result, row.names=F)
```

### Exploratory Analysis
Questions to consider :
- what are the distribution of words frequencies ?
     + analysis with and without stopwords ?
- what are the frequencies of 2-grams and 3-grams ?
- How many unique words in a frequency sorted dictionary to cover 50% of all words instance ? 90% ?
- how to increase the coverage ? 
     + analysis without stemming
     + try with stemming in order to increase coverage
- how evaluate how many words from foreign languages ?


#### Distribution of frequencies of words
Term Document Matrix is a table containing the frequency of words.  
```{r, tdm}
tdm <- TermDocumentMatrix(twitClean)
tdm
```
```{r, matrixTdm, results="asis"}
cleanDF <- function(tdm) {
    # take a tdm and return a dataframe with words/ngrams and ordered frequencies
    m <- as.matrix(tdm)
    v <- sort(rowSums(m), decreasing=T)
    d <- data.frame(word=names(v), freq=v, percent=v/sum(v)*100,
                    cumulative_percent=cumsum(v/sum(v)*100))
    d
}

d <- cleanDF(tdm)
knitr::kable(head(d), row.names=F)
knitr::kable(tail(d), row.names=F)
```
```{r, histo}
breakNb <- 1:d[1,]$freq
hist(d$freq, breaks=breakNb, freq=FALSE, col="lightgreen", border="black",
     main="Distribution of frequencies", xlab="words frequencies")
```

#### Minimum unique words to cover distribution
```{r, coverWords, results="asis"}
# could do a function to choose the coverage value.
plot(d$cumulative_percent, pch=20, xlab="number of unique words", ylab="words coverage (%)")
abline(h=50, col="orange")
abline(v=which(d$cumulative_percent > 50)[1], col="orange")
abline(h=90, col="red")
abline(v=which(d$cumulative_percent > 90)[1], col="red")

index50 <- which(d$cumulative_percent > 50)[1]
index90 <- which(d$cumulative_percent > 90)[1]

coverWords <- data.frame(percent50=index50, percent90=index90)
row.names(coverWords) <- "number of unique words"
knitr::kable(coverWords)
```

#### Plot most frequents words

```{r, mostFrequent, fig.align="center"}
barplot(d[1:30,]$percent, names.arg=d[1:30,]$word, col="lightblue", las=2,
        main="30 most frequent words", ylab="word frequencies (%)")
```

#### Generate Wordcloud
Visual funny representation, use the *wordcloud2()* function of the wordcloud2 package.
```{r, wordcloud, message=FALSE, fig.align="center"}
library(wordcloud2)
d <- cleanDF(tdm)
wordcloud2(d, size=0.6)
```

### 2 and 3-grams
Use the *weka()* function of the RWeka package with tm().

```{r, ngram, results="asis", cache=TRUE}

tdmNgram <- function(corpus, nb=2){
    # take a corpus, min and max length of ngram, and return a tdm of ngrams
    library(RWeka)
    library(tm)
    options(mc.cores=1)
    # http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka/20251039#20251039
    ngramControl <- function(x) {
        RWeka::NGramTokenizer(x, Weka_control(min=nb, max=nb))
    }
    tdm <- TermDocumentMatrix(corpus, control=list(tokenize=ngramControl))
    tdm
}

twoGram <- tdmNgram(twitClean, 2)
knitr::kable(head(cleanDF(twoGram)), row.names=F)

threeGram <- tdmNgram(twitClean, 3)
knitr::kable(head(cleanDF(threeGram)), row.names=F)
```

## tidytext package
***
Source : [tidytextmining.com](http://tidytextmining.com/)

Good to use method like dplyr, very easy to use with pipe. Can be directly piped to ggplot2.  
Datasets must be a dataframe to use the `unnest_tokens()` function.  
Has to write number of lines in a column, then the text dataset in an other, unless error with 'indice hors limite'.

### General use

```{r}
library(tidytext)
library(dplyr)
library(data.table)
twitTest <- sample(twit, 20000, replace=F)
head(twitTest)
dfTwit <- data.table(line=1:20000, text=twitTest)
str(dfTwit)
print(object.size(dfTwit), units="auto", quote=FALSE)

tidyText <- dfTwit %>% unnest_tokens(word, text)
tidyText <- tidyText %>% filter(!grepl('[0-9]', word))

str(tidyText)
head(tidyText)
print(object.size(tidyText), units="auto", quote=FALSE)
```

Note that :

- punctuation has been removed
- unnest_tokens converts the token to lower case.
- numbers are removed with the `filter()` and `grepl()` functions, and a regexp.

BETTER TO REMOVE NUMBERS, STOPWORDS, ABBREVIATIONS, ETC WITH QDAP PAQUAGE...

Many options available to tweak default behaviour.
Easy to use tiny method like dplyr, tidyr, ggplot2 and broom.  


### Stopwords
Use `anti_join()` function with the stop_words data.
```{r}
data(stop_words)
stopText <- tidyText %>% anti_join(stop_words)
dim(stopText)
head(stopText)
print(object.size(stopText), units="auto", quote=FALSE)
```

### Most commun words 
Use `count()` function.
```{r}
freqWord <- stopText %>% count(word, sort=T)
head(freqWord)
```

Easy to plot with pipe and tidy dataset :
```{r}
library(ggplot2)
data(stop_words)
tidyText %>% 
    anti_join(stop_words) %>%
    count(word, sort=TRUE) %>%
    filter(n>150) %>%
    mutate(word=reorder(word,n)) %>%
    ggplot(aes(word,n)) +
    geom_bar(stat="identity", fill="green", color="black", alpha=0.6) +
    xlab(NULL) +
    theme(axis.text.y = element_text(size=10)) +
    coord_flip()
```
```{r, wordcloudTT, message=FALSE, fig.align="center"}
library(wordcloud)
library(RColorBrewer)
countTidyText <- tidyText %>% anti_join(stop_words) %>%
    count(word, sort=T)
countTidyText
# wordcloud(as.data.frame(countTidyText), size=0.6)
wordcloud(words=countTidyText$word, freq=countTidyText$n, max.words=150, random.order=F, rot.per=0.4, scale=c(5,0.5), colors=brewer.pal(8,"Dark2"))
```

### Relations between words

#### N-grams

N-grams con be created with the token option :

```{r}
bigrams <- dfTwit %>% 
    unnest_tokens(bigram, text, token="ngrams", n=2) 
bigrams <- bigrams %>% filter(!grepl('[0-9]', bigram))
str(bigrams)
head(bigrams)
```
Now we can examine the most common bigrams :
```{r}
bigrams %>% count(bigram, sort=T)
```

And the most common bigrams without stopwords : use the `separate()` function of the `tidyr` package, which split column into multiple based on a separator :
```{r}
library(tidyr)
separate_bigrams <- bigrams %>% 
    separate(bigram, c("word1","word2"), sep=" ")
filtered_bigrams <- separate_bigrams %>%
    filter(!word1 %in% stop_words$word) %>%
    filter(!word2 %in% stop_words$word)
# new count
filtered_bigrams %>% count(word1, word2, sort=TRUE)
```

Then `unite()` from the `tidyr` package to merge column and plot a wordcloud of most commons bigrams.
```{r}
united_bigrams <- filtered_bigrams %>%
    unite(bigram, word1, word2, sep=" ") 
counted_bigrams <- united_bigrams %>%
    count(bigram, sort=TRUE)
wordcloud(words=counted_bigrams$bigram, freq=counted_bigrams$n, max.words=150, random.order=F, rot.per=0.35, scale=c(3,0.5), colors=brewer.pal(8,"Dark2"))
```

#### Network of bigrams
Network of bigrams can be drawn with `graph_from_data_frame()` function from the `igraph` package, and `ggraph()` from `ggraph` package.
```{r}
library(igraph)
filtered_bigrams
graph_bigrams <- filtered_bigrams %>%
    count(word1, word2, sort=TRUE) %>%
    filter(n>=10) %>%
    graph_from_data_frame()
graph_bigrams

library(ggraph)
a <- grid::arrow(type="open", angle=15, length=unit(.15, "inches"))
ggraph(graph_bigrams, layout="fr") +
    geom_edge_link(aes(edge_alpha=n), show.legend=FALSE, arrow=a) +
    geom_node_point(color="lightgreen", size=2) +
    geom_node_text(aes(label=name), vjust=1, hjust=1) +
    theme_void()
```


## quanteda package
***
Source : [get started with quanteda](https://cran.r-project.org/web/packages/quanteda/vignettes/quickstart.html)
Another package to create and work with corpus.




## Terms associations

### Explore associations

```{r}
findFreqTerms(tdm, lowfreq=10)
findAssocs(tdm, terms="people", corlimit=0.6)
```
