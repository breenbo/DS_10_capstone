---
title: "Text Prediction - Milestone report"
output: 
    html_document: 
        includes:
            after_body: custom/footer.html
        toc: true 
        toc_depth: 3
        toc_float: true
        highlight: pygments
        css: custom/custom.css
---


```{r globalOptions, echo=FALSE}
knitr::opts_chunk$set(comment=NA, echo=FALSE, message=FALSE)
```
```{r, libraries}
library(doMC) # multicore computation
library(stringr) # wordcount
library(tm)
library(SnowballC)
library(RWeka) # n-grams
library(wordcloud2) # wordcloud
```
```{r, functionResume, echo=FALSE}
library(doMC)
registerDoMC(cores=6) # parallel computation

overview <- function(texte) {
    data.frame(total.Lines=length(texte), total.words=sum(str_count(texte,'\\w+')), mean.word.by.line=mean(str_count(texte,'\\w+')), sd.of.mean=sd(str_count(texte,'\\w+')), max.word.by.line=max(str_count(texte,'\\w+')))
}

# overview for multiple files
multiOverview <- function(liste) {
    # take a list of character containing names of text variables, then return a dataframe of stats
    dataSumList <- lapply(liste, function(x) overview(get(x)))
    len <- length(liste)
    dataSum <- data.frame()
    for(i in 1:len) {
        dataSum <- rbind(dataSum, dataSumList[[i]])
    }
    row.names(dataSum) <- liste
    dataSum
}

# preprocess text file
tidyNGram <- function(texte, lang="en", stopword=TRUE, stemwords=FALSE, n.gram=1) {
    # take a text file, and return a possibly clean and stemmed tidy table depending if chosen options 
    library(data.table)
    library(tm)
    library(tidytext)
    library(dplyr)
    texte <- removeNumbers(texte)
    texte <- tolower(texte)
    if(stopword==FALSE){
        texte <- removeWords(texte, stopwords(lang))
    }
    len <- length(texte)
    textFile <- data.table()
    textFile <- data.table(line=1:len, text=texte)
    tidyText <- textFile %>% 
        unnest_tokens(ngram, text, token="ngrams", n=n.gram)
    tidyText
}

countPlot <- function(tidyText, nb=75){
    # take a tidyText a plot the n most commons words or ngrams with ggplot
    library(dplyr)
    library(ggplot2)
    tidyText %>%
        count(ngram, sort=TRUE) %>%
        mutate(ngram=reorder(ngram,n)) %>%
        slice(1:nb) %>%
        ggplot(aes(ngram,n)) +
        geom_bar(stat="identity", fill="lightgreen", color="green", alpha=0.6) +
        xlab(NULL) +
        theme(axis.text.y = element_text(size=8)) +
        coord_flip()
}

wordCloudPlot <- function(tidyText, nb=100) {
    # take a tidyText and plot a wordCloud of n most common words or ngrams
    library(wordcloud)
    library(RColorBrewer)
    tidyText <- tidyText %>% count(ngram, sort=TRUE)
    wordcloud(words=tidyText$ngram, freq=tidyText$n, max.words=nb, random.order=F, rot.per=0.4, scale=c(5,0.5), colors=brewer.pal(8,"Dark2"))
}

cumulativePlot <- function(tidyText, n1=50, n2=90) {
    # take a tidyText and plot a cumulative plot with nb of unique words to cover n1 and n1 % of total words
    library(dplyr)
    tidyText <- tidyText %>% count(ngram, sort=TRUE) %>%
        mutate(ngram=reorder(ngram,n)) %>%
        mutate(percent=n/sum(n)*100) %>%
        mutate(cumulative_percent=cumsum(percent))

    x1 <- which(tidyText$cumulative_percent>n1)[1]
    x2 <- which(tidyText$cumulative_percent>n2)[1]

    tidyText %>%
        ggplot(aes(1:nrow(tidyText), cumulative_percent)) +
        geom_point(size=0.2) + xlab("Number of unique words") +
        ylab("Cumulative percentage (%)") +
        geom_hline(yintercept=n1, col="orange") +
        geom_vline(xintercept=x1, col="orange") +
        geom_hline(yintercept=n2, col="green") +
        geom_vline(xintercept=x2, col="green") +
        scale_x_continuous(breaks=c(x1,x2))
}
```

# Executive summary

In order to build a predictive algorithm for typed text, three datasets must be analysed to discover relationships between words and group of words.
These datasets contain several sentences and millions of words, and cover three semantics fields :

- a twitter dataset, with short sentences and abbreviations,
- a blog dataset, with more longer sentences and a different vocabulary,
- a news datasets, with another different vocabulary.


After some cleaning by removing "noise" (punctuation, typo, uppercase, numbers and whitespaces), the most frequents words will be plotted with barplots and cloudwords for each datasets.
Note that, unusualy, the common words aren't removed, as people write them and wish to have predictions after them.
The datasets aren't merged too, in order to propose a prediction for each semantic field covered by the three datasets.  
Finally, this choices will lead to the next step of the project : building the predictive algorithm.

## Decision to make

1. sample datasets ?
1. Separate datasets ?
      - twit are short message with lots of typo, and news / blogs are more developed text
      - train model on different purposes, for different users.  
        

1. Remove stopwords ?
      - People write them, so they may need a prediction for stopwords.  
      - Try an anlysis without removing to see what's happening.


1. Stemming or not ?
      - Good to unite declinaison of words, and remove some typos ?

# Datas overview

## What are the datas ?
Example of the datas provided and some stats about the number of words, which are of interest for predictive text purposes.
```{r, readData, cache=TRUE}
library(tm)
twit <- readLines("../text/en_US.twitter.txt", skipNul=TRUE)
blog <- readLines("../text/en_US.blogs.txt", skipNul=TRUE)
nouvelles <- readLines("../text/en_US.news.txt", skipNul=TRUE)
```

Example of lines for each datasets are shown below :  
```{r, sample, cache=TRUE}
twit[1]
blog[1]
nouvelles[1]
```


Quick exploratory of the numbers of words for each lines and for each datasets.
```{r, overview, results="asis", cache=TRUE}
# function to view some stats
liste <- c("twit", "blog", "nouvelles")
datasum <- multiOverview(liste)
# view results
library(knitr)
knitr::kable(datasum, format.args=list(big.mark=","), digits=2)
```


As numbers of words are quite important, the work will be done on a sample with a good representativeness of the total datas (`power.test()` function).
The following hypothesis are done too :

- the vocabulary and the semantics fields are different between each datasets
  - the twit dataset should contains more typo and abbreviations than the two others, because of the character limitation of twitter
    
As people are writing in each semantic field, the datasets will not be merged in order to predict words for each field.

### DO A POWER TEST AROUND MEAN TO CHOOSE SAMPLE SIZE WITH CONFIDENCE OF 95%.

Select random sample for testing process and functions :
```{r, testCorpus}
library(tm)
set.seed(2704)
twitTest <- sample(twit, 20000, replace=T)
```

## PreProcessing - cleaning datasets
In order to show the most frequent words, a cleaning must be done to avoid noise with uppercase, punctuation, typos or numbers.

### USE CLEAN FUNCTION, DON'T REMOVE STOPWORDS IN BEGINNING, TRY STEMMING AFTER

Detailed and teaching process for a limited testing dataset (twitCtest). Use functions at the end of document to process on full datasets.


To illustrate before and after the processing :

# Exploratory Analysis

## Questions to consider

- what are the distribution of words frequencies ?
     + analysis with and without stopwords ?
- what are the frequencies of 2-grams and 3-grams ?
- How many unique words in a frequency sorted dictionary to cover 50% of all words instance ? 90% ?
- how to increase the coverage ? 
     + analysis without stemming
     + try with stemming in order to increase coverage
- how evaluate how many words from foreign languages ?

## Frequencies of words and wordcloud

### Distribution of frequencies of words
Document matrix is a table containing the frequency of words.  
Use *TermDocumentMatrix()* from the tm package.
```{r, matrixTdm, results="asis", eval=FALSE}
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing=T)
d <- data.frame(word=names(v), freq=v, percent=v/sum(v)*100,
                cumulative_percent=cumsum(v/sum(v)*100))
knitr::kable(head(d), row.names=F)
knitr::kable(tail(d), row.names=F)
```

```{r, histo, eval=FALSE}
breakNb <- 1:d[1,]$freq
hist(d$freq, breaks=breakNb, freq=FALSE, col="lightgreen", border="black",
     main="Distribution of frequencies", xlab="words frequencies")
```

### Minimum unique words to cover distribution
```{r, coverWords, results="asis", eval=FALSE}
```

### Plot most frequents words

```{r, mostFrequent, fig.align="center", eval=FALSE}
```

### Generate Wordcloud
Visual funny representation, use the *wordcloud2()* function of the wordcloud2 package.
```{r, wordcloud, message=FALSE, fig.align="center", eval=FALSE}
```

## 2 and 3-grams
Use the *ngram()* function of the ngram package.  
Use the *weka()* function of the RWeka package with tm().

```{r, ngram, results="asis", cache=TRUE}

```

## Terms associations

### Explore associations

```{r}
```

# Sources
Source : [tidytextmining.com](http://tidytextmining.com/)


# Code Resume

## Libraries

## Functions

## Datas Summaries

