---
title: "Text Prediction - Milestone report"
output: 
    html_document: 
        includes:
            after_body: custom/footer.html
        toc: true 
        toc_depth: 3
        toc_float: true
        highlight: pygments
        css: custom/custom.css
---


```{r globalOptions, echo=FALSE}
knitr::opts_chunk$set(comment=NA, echo=FALSE, message=FALSE, warning=FALSE)
```
```{r, libraries}
library(cowplot) # mutiple ggplot
library(data.table)
library(doMC) # multicore computation
library(dplyr)
library(ggplot2)
library(ggraph) # plot words relationship
library(igraph) # plot words relationship
library(knitr)
library(qdap) # cleaning text
library(RColorBrewer)
library(stringr) # wordcount
library(tidyr) # separate words of ngrams into columns
library(tidytext) # for tidy analysis of texts
library(wordcloud)
```
```{r, functionResume, echo=FALSE}
library(doMC)
registerDoMC(cores=6) # parallel computation

############################################################
# OVERVIEW
############################################################
overview <- function(texte) {
    data.frame(total.Lines=length(texte), total.words=sum(str_count(texte,'\\w+')),
               mean.word.by.line=mean(str_count(texte,'\\w+')),
               sd.of.mean=sd(str_count(texte,'\\w+')),
               max.word.by.line=max(str_count(texte,'\\w+')))
}

# overview for multiple files
multiOverview <- function(liste) {
    # take a list of character containing names of text variables, then return a 
    # dataframe of stats
    dataSumList <- lapply(liste, function(x) overview(get(x)))
    len <- length(liste)
    dataSum <- data.frame()
    for(i in 1:len) {
        dataSum <- rbind(dataSum, dataSumList[[i]])
    }
    row.names(dataSum) <- liste
    dataSum
}

############################################################
# CREATE TIDY NGRAMS 
############################################################
tidyNGram <- function(texte, lang="en", rmStopword=FALSE, stemwords=TRUE, n.gram=1) {
    # take a text file, and return a possibly clean without stopwords and stemmed 
    # tidy table depending if chosen options 
    library(data.table)
    library(tidytext)
    library(dplyr)
    library(qdap)
    # cleaning sentences
    texte <- texte %>% 
        replace_contraction() %>%
        tolower() %>%
        replace_number(remove=TRUE) %>%
        replace_ordinal(remove=TRUE) %>%
        replace_symbol() %>% replace_abbreviation()

    # remove stopwords option
    if(rmStopword==TRUE){
        texte <- rm_stopwords(texte, separate=FALSE)
    } 
    # stemming option
    if(stemwords==TRUE){
        texte <- stemmer(texte)
    }
    # transforms list in tidy table
    len <- length(texte)
    textFile <- data.table()
    textFile <- data.table(line=1:len, text=texte)
    tidyText <- textFile %>% 
        unnest_tokens(ngram, text, token="ngrams", n=n.gram)
    tidyText
}

############################################################
# PLOT MOST COMMON NGRAMS
############################################################
countPlot <- function(tidyText, nb=50){
    # take a tidyText a plot the n most commons words or ngrams with ggplot
    library(dplyr)
    library(ggplot2)
    nbgram <- str_count(tidyText$ngram[1], '\\w+')
    if(nbgram==1){
        graphTitle <- paste(nb, "most commons words.")
    } else {
        graphTitle <- paste(nb, " most commons ",nbgram,"-grams", sep="")
    }
    tidyText %>%
        count(ngram, sort=TRUE) %>%
        mutate(ngram=reorder(ngram,n)) %>%
        mutate(percent=n/sum(n)*100) %>%
        # splitting to plot nb words
        slice(1:nb) %>%
        # plot the plot
        ggplot(aes(ngram,percent)) +
        geom_bar(stat="identity", fill="lightgreen", color="black") +
        xlab(NULL) +
        ggtitle(graphTitle) +
        theme(axis.text.y = element_text(size=8)) +
        coord_flip()
}

############################################################
# PLOT WORDCLOUD
############################################################
wordCloudPlot <- function(tidyText, nb=100) {
    # take a tidyText and plot a wordCloud of n most common words or ngrams
    library(wordcloud)
    library(RColorBrewer)
    tidyText <- tidyText %>% count(ngram, sort=TRUE)
    wordcloud(words=tidyText$ngram, freq=tidyText$n, max.words=nb, random.order=F,
              rot.per=0.35, scale=c(5,0.5), colors=brewer.pal(8,"Dark2"))
}

############################################################
# PLOT NGRAM COVERAGE
############################################################
coverWords <- function(tidyText, n1=50, n2=90) {
    # take a tidyText and return a data.frame with numbers of unique words to describe 
    # n1 and n2 % of the total dataset.
    library(dplyr)
    tidyText <- tidyText %>% count(ngram, sort=TRUE) %>%
        mutate(ngram=reorder(ngram,n)) %>%
        mutate(percent=n/sum(n)*100) %>%
        mutate(cumulative_percent=cumsum(percent))

    # select first row which are greater than n1 and n2
    x1 <- which(tidyText$cumulative_percent>n1)[1]
    x2 <- which(tidyText$cumulative_percent>n2)[1]
    results <- data.frame(x1,x2, dim(tidyText)[1])

    name.x1 <- paste(n1,"percent")
    name.x2 <- paste(n2,"percent")

    # part to chose if words or ngrams (for labs and title)
    nbgram <- str_count(tidyText$ngram[1], '\\w+')
    if(nbgram==1){
        names(results) <- c(name.x1, name.x2, 'total unique words')
        row.names(results) <- "Number of unique words"
    } else {
        name.x3 <- "total unique ngrams"
        names(results) <- c(name.x1, name.x2, name.x3)
        row.names(results) <- paste("Number of unique ",nbgram,"-grams", sep="")
    }
    results
}

cumulativePlot <- function(tidyText, n1=50, n2=90) {
    # take a tidyText and plot a cumulative plot with nb of unique words to cover 
    # n1 and n1 % of total words
    library(dplyr)
    tidyText <- tidyText %>% count(ngram, sort=TRUE) %>%
        mutate(ngram=reorder(ngram,n)) %>%
        mutate(percent=n/sum(n)*100) %>%
        mutate(cumulative_percent=cumsum(percent))

    # select first row which are greater than n1 and n2
    x1 <- which(tidyText$cumulative_percent>n1)[1]
    x2 <- which(tidyText$cumulative_percent>n2)[1]
    x3 <- dim(tidyText)[1]

    # part to chose if words or ngrams (for labs and title)
    nbgram <- str_count(tidyText$ngram[1], '\\w+')
    if(nbgram==1){
        xLab <- "Number of unique words"
        graphTitle <- "Coverage of unique words"
    } else {
        xLab <- paste("Number of unique ",nbgram,"-grams", sep="")
        graphTitle <- paste("Coverage of unique ",nbgram,"-grams", sep="")
    }

    tidyText %>%
        ggplot(aes(1:nrow(tidyText), cumulative_percent)) +
        geom_point(size=0.2) + xlab(NULL) + ggtitle(graphTitle) +
        ylab("Cumulative percentage (%)") +
        geom_hline(yintercept=n1, col="orange") +
        geom_vline(xintercept=x1, col="orange") +
        geom_hline(yintercept=n2, col="green") +
        geom_vline(xintercept=x2, col="green") +
        geom_vline(xintercept=x3, col="black") +
        scale_x_continuous(breaks=c(x1,x2,x3)) +
        theme(axis.text.x=element_text(angle=30, hjust=1)) +
        scale_y_continuous(breaks=c(n1,n2,100))
}
############################################################
# PLOT RELATIONSHIP BETWEEN NGRAMS
############################################################
relationPlot <- function(tidyText, nb=100) {
    library(igraph)
    library(ggraph)
    library(stringr)
    library(tidyr)
    # take a tidy ngram and plot relationship between the nb most common words
    # separate column and prepare for plotting
    nbCol <- str_count(tidyText$ngram[1], '\\w+')
    name <- NULL
    for(i in 1:nbCol){
        name <- c(name,paste("word",i, sep=""))
    }
    tidyText <- tidyText %>% 
        count(ngram, sort=T) %>%
        # separate words to plot relationship
        separate(col=ngram, into=name, sep=" ") %>%
        # split to plot only nb words
        slice(1:nb) %>%
        graph_from_data_frame()

    # plot relationship between words
    a <- grid::arrow(type="open", angle=15, length=unit(.15, "inches"))
    ggraph(tidyText, layout="fr") +
        geom_edge_link(aes(edge_alpha=n), show.legend=FALSE, arrow=a) +
        geom_node_point(color="lightgreen", size=2) +
        geom_node_text(aes(label=name), repel=TRUE, color="red") +
        theme_void()
}
```

# Executive summary

In order to build a predictive algorithm for typed text, three datasets must be analysed to discover relationships between words and group of words.
These datasets contain several sentences and millions of words, and cover three semantics fields :

- a twitter dataset, with short sentences and abbreviations,
- a blog dataset, with more longer sentences and a different vocabulary,
- a news datasets, with another different vocabulary.


After some cleaning by removing "noise" (punctuation, typo, uppercase, numbers and whitespaces), the most frequents words will be plotted with barplots and cloudwords for each datasets.  
Note that, unusualy :

- the common words aren't removed, as people write them and wish to have predictions after them, but negatives words "not" is removed as it doesn't change the possible predictions.
- words are stemmed (shortened to their root), in order to have some predictions as soon as the root of the word is typed.
- the raws datasets aren't merged, but a random sample of each of them is taken, in order to cover the three semantics fields, then merged.


Finally, this choices will lead to the next step of the project : building the predictive algorithm.  
All code is available at the end of the report.

## Decision to make

1. sample datasets ?
1. Separate datasets ?
      - twit are short message with lots of typo, and news / blogs are more developed text
      - train model on different purposes, for different users.  
        

1. Remove stopwords ?
      - People write them, so they may need a prediction for stopwords.  
      - Try an anlysis without removing to see what's happening.


1. Stemming or not ?
      - Good to unite declinaison of words, and remove some typos ?
    - As people has written the root of the words, predictions could be made.


# Datas overview

## What are the datas ?
Example of the datas provided and some stats about the number of words, which are of interest for predictive text purposes.
```{r, readData, cache=TRUE}
twit <- readLines("../text/en_US.twitter.txt", skipNul=TRUE)
blog <- readLines("../text/en_US.blogs.txt", skipNul=TRUE)
news <- readLines("../text/en_US.news.txt", skipNul=TRUE)
```

Example of lines for each datasets are shown below :  
```{r, sample, cache=TRUE}
twit[1]
blog[1]
news[1]
```


Quick exploratory of the numbers of words for each lines and for each datasets.
```{r, overview, results="asis", cache=TRUE}
# function to view some stats
liste <- c("twit", "blog", "news")
datasum <- multiOverview(liste)
# view results
library(knitr)
knitr::kable(datasum, format.args=list(big.mark=","), digits=2)
```


As numbers of words are quite important, the work will be done on a sample with a good representativeness of the total datas.
The following hypothesis are done too :

- the vocabulary and the semantics fields are different between each datasets
- the twit dataset should contains more typo and abbreviations than the two others, because of the character limitation of twitter
    
As people are writing in each semantic field, the datasets will not be merged in order to predict words for each field.


Select random sample for testing process and functions :
```{r, tidySets, cache=TRUE}
library(stringr)
set.seed(2704)
twitTest <- sample(twit, 20000, replace=F)
blogTest <- sample(blog, 20000, replace=F)
newsTest <- sample(news, 20000, replace=F)

twitTidy <- tidyNGram(twitTest)
blogTidy <- tidyNGram(blogTest)
newsTidy <- tidyNGram(newsTest)
```
```{r, tidyStop, cache=TRUE}
twitStopTidy <- tidyNGram(twitTest, rmStopword=TRUE)
blogStopTidy <- tidyNGram(blogTest, rmStopword=TRUE)
newsStopTidy <- tidyNGram(newsTest, rmStopword=TRUE)
```
```{r, uniqueSample}
uniqueSample <- rbind(twitTidy, blogTidy, newsTidy)
```

## PreProcessing - cleaning datasets
In order to show the most frequent words, a cleaning must be done to avoid noise with uppercase, punctuation, typos or numbers.  
All the code of the cleaning is available at the end of the report.


# Exploratory Analysis

## Questions to consider

- what are the distribution of words frequencies ?
     + analysis with and without stopwords ?
- what are the frequencies of 2-grams and 3-grams ?
- How many unique words in a frequency sorted dictionary to cover 50% of all words instance ? 90% ?
- how to increase the coverage ? 
     + analysis without stemming
     + try with stemming in order to increase coverage
- how evaluate how many words from foreign languages ?

## Frequencies of words and wordcloud

### Most commons words
```{r, histo}
countPlot(uniqueSample)
wordCloudPlot(uniqueSample, nb=200)
```


As seen below the results aren't very interesting, because the most commons words are... common words, like 'the' or 'a' or 'and'. But they are needed as people type them and might need a prediction for them.
Studying the distribution of only one words is not a solution for a predictive software.  


### Minimum unique words to cover distribution
Some interesting results : the numbers of unique words to describe 90 % of the total words is not so important, as seen in the following plots and tables.

```{r, coverWords}
# cumulativePlot(twitTidy)
# cumulativePlot(blogTidy)
# cumulativePlot(newsTidy)
cumulativePlot(uniqueSample)
```

```{r, summary}
# twitCover <- coverWords(twitTidy)
# blogCover <- coverWords(blogTidy)
# newsCover <- coverWords(newsTidy)
# uniqueCover <- coverWords(uniqueSample)

# results <- rbind(twitCover, blogCover, newsCover, uniqueCover)
# row.names(results) <- c("twitter", "blog","news", "merged")
knitr::kable(coverWords(uniqueSample))
```

It might be possible to have a good prediction for only few uniques words, as they cover most of the uses.  
The next step is to study the associations of words in the dataset, to begin to think of a predictive model.


## Terms associations

### 2 and 3-grams
To study associations of words, it's useful to check 'n-grams', which are groups of n words. As for unique words, the mosts commons n-grams and the minimum unique n-grams to cover 90 % of the distribution will be studied.  
Then a map of words associations can be plotted.

```{r, bigram, cache=TRUE}
twitBigram <- tidyNGram(twitTest, n=2)
blogBigram <- tidyNGram(blogTest, n=2)
newsBigram <- tidyNGram(newsTest, n=2)
```
```{r, uniqueBigram}
uniqueBigram <- rbind(twitBigram, blogBigram, newsBigram)
```
```{r, trigram, cache=TRUE}
twitTrigram <- tidyNGram(twitTest, n=3)
blogTrigram <- tidyNGram(blogTest, n=3)
newsTrigram <- tidyNGram(newsTest, n=3)
```
```{r, uniqueTrigram}
uniqueTrigram <- rbind(twitTrigram, blogTrigram, newsTrigram)
```

Distributions of most commons bigrams and trigrams, without any filter :

```{r, countPlot}
# countPlot(uniqueBigram)
# wordCloudPlot(uniqueBigram)
# countPlot(uniqueTrigram)
# wordCloudPlot(uniqueTrigram)
plot_grid(countPlot(uniqueBigram), countPlot(uniqueTrigram), align='h')
```

Another way to represent words associations are shown below : associations seems to be more "precise" as n increase.

```{r, relationPlot, cache=TRUE}
# need a bigram at least !
# by default, the relationPlot for 100 most common ngrams.
relationPlot(uniqueBigram, nb=150)
relationPlot(uniqueTrigram, nb=150)
```

Some interesting results, by filtering bigrams beginning by 'love', then filtering trigrams by 'love you'. 

```{r, exampleCountPlot}
loveBi <- uniqueBigram %>% filter(grepl('^love ', ngram))
loveBi <- countPlot(loveBi)
loveTri <- uniqueTrigram %>% filter(grepl('^love you ', ngram))
loveTri <- countPlot(loveTri)
plot_grid(loveBi, loveTri, align='h')
```

Some predictions possibilities begin to appear, for example by filtering bigrams by the first word typed by the user, then filtering trigrams by the two words typed by user, and so on, and choosing the first three results of each.  
But a problem appears too : the numbers of ngrams necessary to cover the distribution increase with n, as shown in the followings plots.

```{r, ngramPlot, cache=TRUE}
plotTri <- cumulativePlot(uniqueTrigram) + ylab(NULL)
plot_grid(cumulativePlot(uniqueBigram), plotTri, align='h')
# plot_grid(cumulativePlot(uniqueBigram), cumulativePlot(uniqueTrigram))
resultCover <- rbind(coverWords(uniqueBigram), coverWords(uniqueTrigram))
knitr::kable(resultCover)
# test for cache
# knitr::kable(coverWords(uniqueBigram))
# knitr::kable(coverWords(uniqueTrigram))
```


# Future
All the difficulty is to have the most precise prediction as possible, but also as quick as possible.  
The preliminary study show that a predictive model could work : as the user type n words, find the most common (n+1)grams corresponding to typed words.  
So the future studies consists of find the best (n+1)grams to permit accuracy and quick response.

## Sources
Source : [tidytextmining.com](http://tidytextmining.com/)


# Code Resume

## Libraries
```{r, ref.label="libraries", eval=FALSE, echo=TRUE}
```

## Functions
```{r, ref.label="functionResume", eval=FALSE, echo=TRUE}
```
