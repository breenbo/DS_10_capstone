---
title: "Text Prediction - Milestone report"
output: 
    html_document: 
        includes:
            after_body: custom/footer.html
        toc: true 
        toc_depth: 3
        toc_float: true
        highlight: pygments
        css: custom/custom.css
---


```{r globalOptions, echo=FALSE}
knitr::opts_chunk$set(comment=NA, echo=FALSE, message=FALSE)
```
```{r, libraries}
library(doMC) # multicore computation
library(stringr) # wordcount
library(tm)
library(SnowballC)
library(RWeka) # n-grams
library(wordcloud2) # wordcloud
```
```{r, functionResume, echo=FALSE}
library(doMC)
registerDoMC(cores=6) # parallel computation

# quick overwiew of datas
# overviewfull <- function(texte) {
    # data.frame(lines=length(texte), meanChar=mean(nchar(texte)), sdChar=sd(nchar(texte)), minChar=min(nchar(texte)), maxChar=max(nchar(texte)))
# }

overview <- function(texte) {
    data.frame(total.Lines=length(texte), total.words=sum(str_count(texte,'\\w+')), mean.word.by.line=mean(str_count(texte,'\\w+')), sd.of.mean=sd(str_count(texte,'\\w+')), max.word.by.line=max(str_count(texte,'\\w+')))
}

# overview for multiple files
multiOverview <- function(liste) {
    # take a list of character containing names of text variables, then return a dataframe of stats
    dataSumList <- lapply(liste, function(x) overview(get(x)))
    len <- length(liste)
    dataSum <- data.frame()
    for(i in 1:len) {
        dataSum <- rbind(dataSum, dataSumList[[i]])
    }
    row.names(dataSum) <- liste
    dataSum
}

# preprocess text file
cleanCorpus <- function(textFile, lang="english", stopwords=FALSE, stemDoc=FALSE) {
    # take a text file or a text variable, and a language, and return a clean and stemmed corpus 
    library(tm)
    library(SnowballC)
    if(length(textFile)==1) {
        # textFile is a path to file must be read
        texte <- readLines(textFile, skipNul=TRUE)
    } else {
        # textFile is a variable already read
        texte <- textFile
    }
    # create corpus with file
    corpus <- VCorpus(VectorSource(texte))
    # clean and stem corpus
    corpus <- tm_map(corpus, content_transformer(tolower))
    corpus <- tm_map(corpus, removePunctuation)
    corpus <- tm_map(corpus, removeNumbers)
    if(stopwords==TRUE){
        corpus <- tm_map(corpus, removeWords, stopwords(lang))
    }
    if(stemDoc==TRUE){
        corpus <- tm_map(corpus, stemDocument, language=lang)
    }
    corpus <- tm_map(corpus, stripWhitespace)
    corpus
}

cleanDF <- function(tdm) {
    # take a tdm and return a dataframe with words/ngrams and ordered frequencies
    m <- as.matrix(tdm)
    v <- sort(rowSums(m), decreasing=T)
    d <- data.frame(word=names(v), freq=v, percent=v/sum(v)*100,
                    cumulative_percent=cumsum(v/sum(v)*100))
    d
}

tdmNgram <- function(corpus, nb=2){
    # take a corpus, min and max length of ngram, and return a tdm of ngrams
    library(tm)
    library(RWeka)
    options(mc.cores=1)
    ngramControl <- function(x) {
        RWeka::NGramTokenizer(x, Weka_control(min=nb, max=nb))
    }
    tdm <- TermDocumentMatrix(corpus, control=list(tokenize=ngramControl))
    tdm
}
```

# Executive summary

In order to build a predictive algorithm for typed text, three datasets must be analysed to discover relationships between words and group of words.
These datasets contain several sentences and millions of words, and cover three semantics fields :

- a twitter dataset, with short sentences and abbreviations,
- a blog dataset, with more longer sentences and a different vocabulary,
- a news datasets, with another different vocabulary.


After some cleaning by removing "noise" (punctuation, typo, uppercase, numbers and whitespaces), the most frequents words will be plotted with barplots and cloudwords for each datasets.
Note that, unusualy, the common words aren't removed, as people write them and wish to have predictions after them.
The datasets aren't merged too, in order to propose a prediction for each semantic field covered by the three datasets.  
Finally, this choices will lead to the next step of the project : building the predictive algorithm.

## Decision to make

1. sample datasets ?
1. Separate datasets ?
      - twit are short message with lots of typo, and news / blogs are more developed text
      - train model on different purposes, for different users.  
        

1. Remove stopwords ?
      - People write them, so they may need a prediction for stopwords.  
      - Try an anlysis without removing to see what's happening.


1. Stemming or not ?
      - Good to unite declinaison of words, and remove some typos ?

# Datas overview

## What are the datas ?
Example of the datas provided and some stats about the number of words, which are of interest for predictive text purposes.
```{r, readData, cache=TRUE}
library(tm)
twit <- readLines("../text/en_US.twitter.txt", skipNul=TRUE)
blog <- readLines("../text/en_US.blogs.txt", skipNul=TRUE)
nouvelles <- readLines("../text/en_US.news.txt", skipNul=TRUE)
```

Example of lines for each datasets are shown below :  
```{r, sample, cache=TRUE}
twit[1]
blog[1]
nouvelles[1]
```


Quick exploratory of the numbers of words for each lines and for each datasets.
```{r, overview, results="asis", cache=TRUE}
# function to view some stats
liste <- c("twit", "blog", "nouvelles")
datasum <- multiOverview(liste)
# view results
library(knitr)
knitr::kable(datasum, format.args=list(big.mark=","), digits=2)
```


As numbers of words are quite important, the work will be done on a sample with a good representativeness of the total datas (`power.test()` function).
The following hypothesis are done too :

- the vocabulary and the semantics fields are different between each datasets
  - the twit dataset should contains more typo and abbreviations than the two others, because of the character limitation of twitter
    
As people are writing in each semantic field, the datasets will not be merged in order to predict words for each field.

### DO A POWER TEST AROUND MEAN TO CHOOSE SAMPLE SIZE WITH CONFIDENCE OF 95%.

Select random sample for testing process and functions :
```{r, testCorpus}
library(tm)
set.seed(2704)
twitTest <- sample(twit, 20000, replace=T)
twitCTest <- VCorpus(VectorSource(twitTest))
twitCTest[[27]]$content
```

## PreProcessing - cleaning datasets
In order to show the most frequent words, a cleaning must be done to avoid noise with uppercase, punctuation, typos or numbers.

### USE CLEAN FUNCTION, DON'T REMOVE STOPWORDS IN BEGINNING, TRY STEMMING AFTER

Detailed and teaching process for a limited testing dataset (twitCtest). Use functions at the end of document to process on full datasets.

```{r}
twitClean <- cleanCorpus(twitCTest)
```

To illustrate before and after the processing :
```{r, compareResults, results="asis"}
result <- data.frame(before=twitCTest[[27]]$content, after=twitClean[[27]]$content)
knitr::kable(result, row.names=F)
```

# Exploratory Analysis

## Questions to consider

- what are the distribution of words frequencies ?
     + analysis with and without stopwords ?
- what are the frequencies of 2-grams and 3-grams ?
- How many unique words in a frequency sorted dictionary to cover 50% of all words instance ? 90% ?
- how to increase the coverage ? 
     + analysis without stemming
     + try with stemming in order to increase coverage
- how evaluate how many words from foreign languages ?

## Frequencies of words and wordcloud

### Distribution of frequencies of words
Document matrix is a table containing the frequency of words.  
Use *TermDocumentMatrix()* from the tm package.
```{r, tdm}
tdm <- TermDocumentMatrix(twitClean)
tdm
```
```{r, matrixTdm, results="asis"}
m <- as.matrix(tdm)
v <- sort(rowSums(m), decreasing=T)
d <- data.frame(word=names(v), freq=v, percent=v/sum(v)*100,
                cumulative_percent=cumsum(v/sum(v)*100))
knitr::kable(head(d), row.names=F)
knitr::kable(tail(d), row.names=F)
```

```{r, histo}
breakNb <- 1:d[1,]$freq
hist(d$freq, breaks=breakNb, freq=FALSE, col="lightgreen", border="black",
     main="Distribution of frequencies", xlab="words frequencies")
```

### Minimum unique words to cover distribution
```{r, coverWords, results="asis"}
# could do a function to choose the coverage value.
plot(d$cumulative_percent, pch=20, xlab="number of unique words", ylab="words coverage (%)")
abline(h=50, col="orange")
abline(v=which(d$cumulative_percent > 50)[1], col="orange")
abline(h=90, col="red")
abline(v=which(d$cumulative_percent > 90)[1], col="red")

index50 <- which(d$cumulative_percent > 50)[1]
index90 <- which(d$cumulative_percent > 90)[1]

coverWords <- data.frame(percent50=index50, percent90=index90)
row.names(coverWords) <- "number of unique words"
knitr::kable(coverWords)
```

### Plot most frequents words

```{r, mostFrequent, fig.align="center"}
barplot(d[1:30,]$percent, names.arg=d[1:30,]$word, col="lightblue", las=2,
        main="30 most frequent words", ylab="word frequencies (%)")
```

### Generate Wordcloud
Visual funny representation, use the *wordcloud2()* function of the wordcloud2 package.
```{r, wordcloud, message=FALSE, fig.align="center"}
library(wordcloud2)
wordcloud2(d[1:200,], size=0.6)
```

## 2 and 3-grams
Use the *ngram()* function of the ngram package.  
Use the *weka()* function of the RWeka package with tm().

```{r, ngram, results="asis", cache=TRUE}

tdmNgram <- function(corpus, nb=2){
    # take a corpus, min and max length of ngram, and return a tdm of ngrams
    library(RWeka)
    library(tm)
    options(mc.cores=1)
    # http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka/20251039#20251039
    ngramControl <- function(x) {
        RWeka::NGramTokenizer(x, Weka_control(min=nb, max=nb))
    }
    tdm <- TermDocumentMatrix(corpus, control=list(tokenize=ngramControl))
    tdm
}

twoGram <- tdmNgram(twitClean, 2)
knitr::kable(head(cleanDF(twoGram)), row.names=F)

threeGram <- tdmNgram(twitClean, 3)
knitr::kable(head(cleanDF(threeGram)), row.names=F)
```

## Terms associations

### Explore associations

```{r}
findFreqTerms(tdm, lowfreq=10)
findAssocs(tdm, terms="people", corlimit=0.6)
```

# Sources : 

- [data.bzh](http://data-bzh.fr/?s=text+mining)
- [whatsupdata.fr](http://whatsupdata.fr/content/le-text-mining-et-la-d%C3%A9couverte-des-principales-%C2%AB-th%C3%A9matiques-%C2%BB-d%E2%80%99un-corpus-de-documents)
- [gentle introduction to text mining using R](https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/)
- [gentle introduction to topic modeling using R](https://eight2late.wordpress.com/?s=topic+model)
- [R code : reading, preprocessing and counting](http://www.katrinerk.com/courses/words-in-a-haystack-an-introductory-statistics-course/schedule-words-in-a-haystack/r-code-the-text-mining-package)

# Code Resume

## Libraries
```{r, ref.label="libraries", echo=TRUE, eval=FALSE}
```

## Functions

```{r, ref.label="functionResume", echo=TRUE, eval=FALSE}
```

## Datas Summaries
```{r, ref.label="readData", echo=TRUE, eval=FALSE}
```
```{r, ref.label="overwiew", echo=TRUE, eval=FALSE}
```

